{
  "model_mapping": {
    "phase_1_domains": {
      "healthcare": {
        "primary_model": "microsoft/DialoGPT-medium",
        "model_path": "models/microsoft_DialoGPT-medium",
        "adapter_path": "models/adapters/healthcare",
        "fallback_model": "microsoft_DialoGPT-medium",
        "fallback_path": "models/microsoft_DialoGPT-medium",
        "model_type": "conversational_lora",
        "parameters": "345M",
        "context_length": 1024,
        "description": "DialoGPT-medium with healthcare LoRA adapter for medical conversations",
        "capabilities": ["medical_guidance", "healthcare_conversations", "empathetic_responses"],
        "status": "training_pending",
        "training_priority": 1,
        "cpu_optimized": true,
        "memory_efficient": true,
        "privacy_level": "maximum",
        "phase": "Phase 1 - Arc Reactor Foundation"
      },
    "business": {
        "primary_model": "microsoft/DialoGPT-medium",
        "model_path": "models/microsoft_DialoGPT-medium",
        "adapter_path": "models/adapters/business",
        "fallback_model": "microsoft_DialoGPT-medium",
        "fallback_path": "models/microsoft_DialoGPT-medium",
        "model_type": "conversational_lora",
        "parameters": "345M",
        "context_length": 1024,
        "description": "DialoGPT-medium with business LoRA adapter for professional conversations",
        "capabilities": ["business_strategy", "professional_communication", "market_insights"],
        "status": "training_pending",
        "training_priority": 2,
        "cpu_optimized": true,
        "memory_efficient": true,
        "phase": "Phase 1 - Arc Reactor Foundation"
    },
    "education": {
        "primary_model": "Qwen/Qwen2.5-3B-Instruct",
        "model_path": "models/Qwen_Qwen2.5-3B-Instruct",
        "adapter_path": "models/adapters/education",
        "fallback_model": "microsoft_DialoGPT-medium",
        "fallback_path": "models/microsoft_DialoGPT-medium",
        "model_type": "instruct_lora",
        "parameters": "3B",
        "context_length": 4096,
        "description": "Qwen2.5-3B-Instruct with education LoRA adapter for advanced learning facilitation",
        "capabilities": ["educational_content", "tutoring", "personalized_learning", "advanced_reasoning"],
        "status": "training_pending",
        "training_priority": 3,
        "cpu_optimized": true,
        "memory_efficient": true,
        "current_training": false,
        "progress": "0% (0/1125 steps)",
        "phase": "Phase 1 - Arc Reactor Foundation",
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      },
      "creative": {
        "primary_model": "Qwen/Qwen2.5-3B-Instruct",
        "model_path": "models/Qwen_Qwen2.5-3B-Instruct",
        "adapter_path": "models/adapters/creative",
        "fallback_model": "microsoft_DialoGPT-medium",
        "fallback_path": "models/microsoft_DialoGPT-medium",
        "model_type": "instruct_lora",
        "parameters": "3B",
        "context_length": 4096,
        "description": "Qwen2.5-3B-Instruct with creative LoRA adapter for advanced artistic collaboration",
        "capabilities": ["creative_writing", "storytelling", "artistic_guidance", "advanced_creativity"],
        "status": "training_pending",
        "training_priority": 4,
        "cpu_optimized": true,
        "memory_efficient": true,
        "phase": "Phase 1 - Arc Reactor Foundation",
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      },
      "leadership": {
        "primary_model": "Qwen/Qwen2.5-3B-Instruct",
        "model_path": "models/Qwen_Qwen2.5-3B-Instruct",
        "adapter_path": "models/adapters/leadership",
        "fallback_model": "microsoft_DialoGPT-medium",
        "fallback_path": "models/microsoft_DialoGPT-medium",
        "model_type": "instruct_lora",
        "parameters": "3B",
        "context_length": 4096,
        "description": "Qwen2.5-3B-Instruct with leadership LoRA adapter for advanced management coaching",
        "capabilities": ["leadership_coaching", "team_management", "decision_support", "strategic_thinking"],
        "status": "training_pending",
        "training_priority": 5,
        "cpu_optimized": true,
        "memory_efficient": true,
        "phase": "Phase 1 - Arc Reactor Foundation",
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      }
    },
    "phase_2_domains": {
      "mental_health": {
        "target_model": "microsoft/Phi-3.5-mini-instruct",
        "gguf_model": "models/gguf/Phi-3.5-mini-instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/mental_health",
        "description": "Emotional wellbeing and psychological support with privacy focus",
        "capabilities": ["therapy_complement", "emotional_support", "self_awareness"],
        "status": "phase_2_planned",
        "privacy_level": "maximum",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "career": {
        "target_model": "microsoft/Phi-3.5-mini-instruct",
        "gguf_model": "models/gguf/Phi-3.5-mini-instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/career",
        "description": "Career development and professional growth guidance",
        "capabilities": ["skill_gap_analysis", "opportunity_identification", "career_planning"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "entrepreneurship": {
        "target_model": "microsoft/Phi-3.5-mini-instruct",
        "gguf_model": "models/gguf/Phi-3.5-mini-instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/entrepreneurship",
        "description": "Startup and business creation support",
        "capabilities": ["business_model_validation", "risk_assessment", "startup_guidance"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "skill_development": {
        "target_model": "Qwen/Qwen2.5-7B-Instruct",
        "gguf_model": "models/gguf/qwen2.5-3b-instruct-q4_0.gguf",
        "model_path": "models/adapters/skill_development",
        "description": "Professional and personal skill building",
        "capabilities": ["adaptive_learning", "progress_tracking", "skill_assessment"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "knowledge_transfer": {
        "target_model": "Qwen/Qwen2.5-7B-Instruct",
        "gguf_model": "models/gguf/qwen2.5-3b-instruct-q4_0.gguf",
        "model_path": "models/adapters/knowledge_transfer",
        "description": "Information sharing and documentation",
        "capabilities": ["knowledge_organization", "training_materials", "documentation"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "visual_arts": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/visual_arts",
        "description": "Design, illustration, and visual creation",
        "capabilities": ["concept_development", "technique_guidance", "visual_creativity"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "content_creation": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/content_creation",
        "description": "Digital content and media production",
        "capabilities": ["content_strategy", "production_optimization", "multi_modal_content"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "personal_organization": {
        "target_model": "microsoft/Phi-3.5-mini-instruct",
        "gguf_model": "models/gguf/Phi-3.5-mini-instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/personal_organization",
        "description": "Time management and productivity",
        "capabilities": ["schedule_optimization", "task_prioritization", "productivity_enhancement"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "relationships": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/relationships",
        "description": "Personal and professional relationship support",
        "capabilities": ["communication_enhancement", "conflict_resolution", "relationship_building"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      },
      "cultural_understanding": {
        "target_model": "Qwen/Qwen2.5-7B-Instruct",
        "gguf_model": "models/gguf/qwen2.5-3b-instruct-q4_0.gguf",
        "model_path": "models/adapters/cultural_understanding",
        "description": "Cross-cultural communication and awareness",
        "capabilities": ["cultural_context", "language_nuance", "cross_cultural_communication"],
        "status": "phase_2_planned",
        "phase": "Phase 2 - Perplexity Intelligence"
      }
    },
    "phase_3_domains": {
      "fitness": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/fitness",
        "description": "Exercise and physical wellness optimization",
        "capabilities": ["workout_planning", "form_coaching", "fitness_tracking"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion"
      },
      "nutrition": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/nutrition",
        "description": "Food science and dietary guidance",
        "capabilities": ["meal_planning", "nutritional_analysis", "dietary_guidance"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion"
      },
      "research": {
        "target_model": "meta-llama/Llama-3.1-8B-Instruct",
        "gguf_model": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/research",
        "description": "Scientific research and discovery support including space technology",
        "capabilities": ["hypothesis_generation", "data_analysis", "research_methodology", "space_tech", "aerospace_engineering"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion",
        "specializations": ["space_technology", "aerospace_research", "astrophysics", "satellite_engineering"]
      },
      "engineering": {
        "target_model": "meta-llama/Llama-3.1-8B-Instruct",
        "gguf_model": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/engineering",
        "description": "Advanced technical problem-solving, space systems, and aerospace engineering",
        "capabilities": ["cad_assistance", "simulation_support", "technical_troubleshooting", "space_systems", "rocket_engineering"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion",
        "specializations": ["space_propulsion", "orbital_mechanics", "spacecraft_design", "mission_planning"]
      },
      "data_science": {
        "target_model": "meta-llama/Llama-3.1-8B-Instruct",
        "gguf_model": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/data_science",
        "description": "Advanced data analysis, machine learning, and space data processing",
        "capabilities": ["statistical_analysis", "pattern_recognition", "ml_guidance", "space_data_analysis", "astronomical_data"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion",
        "specializations": ["space_data_analytics", "satellite_telemetry", "mission_data_processing"]
      },
      "music": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/music",
        "description": "Musical composition and performance",
        "capabilities": ["composition_assistance", "arrangement_ideas", "music_theory"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion"
      },
      "home_management": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/home_management",
        "description": "Household organization and maintenance",
        "capabilities": ["automation_suggestions", "efficiency_improvements", "home_organization"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion"
      },
      "financial_planning": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/qwen2.5-3b-instruct-q4_0.gguf",
        "model_path": "models/adapters/financial_planning",
        "description": "Personal finance and investment guidance",
        "capabilities": ["budget_optimization", "investment_research", "financial_planning"],
        "status": "phase_3_planned",
        "phase": "Phase 3 - Einstein Fusion"
      }
    },
    "phase_4_domains": {
      "innovation": {
        "target_model": "meta-llama/Llama-3.1-8B-Instruct",
        "gguf_model": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/innovation",
        "description": "Advanced technology development, R&D, and breakthrough innovation including space technology",
        "capabilities": ["ideation_enhancement", "patent_research", "innovation_strategy", "space_tech_development", "breakthrough_detection"],
        "status": "phase_4_planned",
        "phase": "Phase 4 - Universal Trinity",
        "specializations": ["space_technology_innovation", "next_gen_propulsion", "interplanetary_systems", "space_colonization_tech"]
      },
      "community": {
        "target_model": "meta-llama/Llama-3.2-7B-Instruct",
        "gguf_model": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
        "model_path": "models/adapters/community",
        "description": "Community building and social engagement",
        "capabilities": ["event_planning", "community_outreach", "social_organization"],
        "status": "phase_4_planned",
        "phase": "Phase 4 - Universal Trinity"
      },
      "social_impact": {
        "target_model": "meta-llama/Llama-3.1-8B-Instruct",
        "gguf_model": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/social_impact",
        "description": "Social good, humanitarian efforts, and global impact including space-based solutions",
        "capabilities": ["impact_measurement", "resource_optimization", "social_innovation", "space_based_solutions"],
        "status": "phase_4_planned",
        "phase": "Phase 4 - Universal Trinity",
        "specializations": ["space_based_earth_monitoring", "satellite_humanitarian_aid", "space_resource_utilization"]
      },
      "global_awareness": {
        "target_model": "meta-llama/Llama-3.1-8B-Instruct",
        "gguf_model": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "model_path": "models/adapters/global_awareness",
        "description": "International affairs, global trends, and space-based global intelligence",
        "capabilities": ["information_synthesis", "trend_analysis", "global_intelligence", "space_based_monitoring"],
        "status": "phase_4_planned",
        "phase": "Phase 4 - Universal Trinity",
        "specializations": ["earth_observation", "climate_monitoring", "global_communications", "space_diplomacy"]
      }
    },
    "universal": {
      "primary_model": "microsoft/DialoGPT-medium",
      "model_path": "models/microsoft_DialoGPT-medium",
      "fallback_model": "microsoft_DialoGPT-medium",
      "fallback_path": "models/microsoft_DialoGPT-medium",
      "model_type": "conversational_base",
      "parameters": "345M",
      "context_length": 1024,
      "description": "Base DialoGPT-medium for general conversations when no domain adapter is available",
      "capabilities": ["general_conversation", "dialogue_generation"],
      "status": "ready",
      "cpu_optimized": true,
      "memory_efficient": true,
      "phase": "Universal Base"
    }
  },
  "available_models": {
    "microsoft_DialoGPT-medium": {
      "path": "models/microsoft_DialoGPT-medium",
      "parameters": "345M",
      "type": "conversational",
      "license": "MIT",
      "status": "ready",
      "context_length": 1024,
      "capabilities": ["conversation", "dialogue_generation"],
      "last_verified": "2025-01-23",
      "cpu_friendly": true,
      "memory_usage": "low",
      "training_base": true,
      "current_usage": "Phase 1 foundation model"
    },
    "microsoft_DialoGPT-large": {
      "path": "models/microsoft_DialoGPT-large",
      "parameters": "762M",
      "type": "conversational",
      "license": "MIT",
      "status": "available",
      "context_length": 1024,
      "capabilities": ["conversation", "dialogue_generation"],
      "last_verified": "2025-01-14",
      "cpu_friendly": true,
      "memory_usage": "medium",
      "note": "Available as fallback but not currently used"
    },
    "microsoft_phi-2": {
      "path": "models/microsoft_phi-2",
      "parameters": "2.7B",
      "type": "general",
      "license": "MIT",
      "status": "available",
      "context_length": 2048,
      "capabilities": ["reasoning", "code_generation", "general_qa"],
      "last_verified": "2025-01-14",
      "cpu_friendly": false,
      "memory_usage": "high",
      "note": "Too memory-intensive for current CPU training"
    }
  },
  "gguf_models": {
    "llama-3.2-1b-instruct-q4_0": {
      "path": "models/gguf/llama-3.2-1b-instruct-q4_0.gguf",
      "parameters": "1B",
      "size": "737MB",
      "type": "instruction_tuned_gguf",
      "license": "Llama 3.2 Community License",
      "status": "ready",
      "context_length": 128000,
      "capabilities": ["instruction_following", "general_reasoning", "creative_tasks"],
      "target_domains": ["visual_arts", "content_creation", "relationships", "fitness", "nutrition", "music", "home_management", "community"],
      "cpu_optimized": true,
      "memory_efficient": true,
      "inference_speed": "fast"
    },
    "qwen2.5-3b-instruct-q4_0": {
      "path": "models/gguf/qwen2.5-3b-instruct-q4_0.gguf",
      "parameters": "3B",
      "size": "1.9GB",
      "type": "instruction_tuned_gguf",
      "license": "Apache 2.0",
      "status": "ready",
      "context_length": 32768,
      "capabilities": ["multilingual", "reasoning", "educational_content", "cultural_understanding"],
      "target_domains": ["skill_development", "knowledge_transfer", "cultural_understanding", "financial_planning"],
      "cpu_optimized": true,
      "memory_efficient": true,
      "inference_speed": "medium"
    },
    "phi-3.5-mini-instruct-q4_k_m": {
      "path": "models/gguf/Phi-3.5-mini-instruct-Q4_K_M.gguf",
      "parameters": "3.8B",
      "size": "2.2GB",
      "type": "instruction_tuned_gguf",
      "license": "MIT",
      "status": "ready",
      "context_length": 128000,
      "capabilities": ["business_reasoning", "professional_communication", "code_generation"],
      "target_domains": ["mental_health", "career", "entrepreneurship", "personal_organization"],
      "cpu_optimized": true,
      "memory_efficient": true,
      "inference_speed": "medium"
    },
    "meta-llama-3.1-8b-instruct-q4_k_m": {
      "path": "models/gguf/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "parameters": "8B",
      "size": "4.6GB",
      "type": "instruction_tuned_gguf",
      "license": "Llama 3.1 Community License",
      "status": "ready",
      "context_length": 128000,
      "capabilities": ["advanced_reasoning", "research_support", "complex_analysis", "space_technology", "engineering"],
      "target_domains": ["research", "engineering", "data_science", "innovation", "social_impact", "global_awareness"],
      "cpu_optimized": true,
      "memory_usage": "high",
      "inference_speed": "slower_but_powerful",
      "specializations": ["space_technology", "aerospace_engineering", "scientific_research", "advanced_analytics"]
    }
  },
  "future_models": {
    "microsoft/Phi-3.5-mini-instruct": {
      "target_domains": ["business", "mental_health", "career", "entrepreneurship", "personal_organization"],
      "parameters": "3.8B",
      "type": "instruction_tuned",
      "license": "MIT",
      "status": "phase_2_target",
      "context_length": 128000,
      "capabilities": ["business_reasoning", "professional_communication", "efficiency"],
      "requires_token": false,
      "priority": "phase_2",
      "note": "Target for Phase 2 professional domains"
    },
    "Qwen/Qwen2.5-7B-Instruct": {
      "target_domains": ["education", "skill_development", "knowledge_transfer", "cultural_understanding"],
      "parameters": "7B",
      "type": "instruction_tuned",
      "license": "Apache 2.0",
      "status": "phase_2_target",
      "context_length": 32768,
      "capabilities": ["educational_content", "multilingual", "tutoring"],
      "requires_token": false,
      "priority": "phase_2",
      "note": "Target for Phase 2 educational domains"
    },
    "meta-llama/Llama-3.2-7B-Instruct": {
      "target_domains": ["healthcare", "creative", "leadership", "visual_arts", "content_creation", "relationships", "fitness", "nutrition", "music", "home_management", "financial_planning", "community"],
      "parameters": "7B",
      "type": "instruction_tuned",
      "license": "Llama 3.2 Community License",
      "status": "phase_2_3_target",
      "context_length": 128000,
      "capabilities": ["advanced_reasoning", "domain_expertise", "safety_focused"],
      "requires_token": true,
      "priority": "phase_2_3",
      "note": "Target for Phase 2-3 specialized domains"
    },
    "meta-llama/Llama-3.2-14B-Instruct": {
      "target_domains": ["research", "engineering", "data_science", "innovation", "social_impact", "global_awareness"],
      "parameters": "14B",
      "type": "instruction_tuned",
      "license": "Llama 3.2 Community License",
      "status": "phase_3_4_target",
      "context_length": 128000,
      "capabilities": ["advanced_reasoning", "complex_analysis", "research_grade", "space_technology"],
      "requires_token": true,
      "priority": "phase_3_4",
      "note": "Target for Phase 3-4 advanced domains including space technology"
    }
  },
  "training_configuration": {
    "current_phase": "Phase 1 - Arc Reactor Foundation",
    "total_domains": 24,
    "phase_1_domains": 5,
    "phase_2_domains": 10,
    "phase_3_domains": 8,
    "phase_4_domains": 4,
    "base_model": "microsoft/DialoGPT-medium",
    "training_method": "LoRA fine-tuning",
    "trainable_parameters": "15.32%",
    "cpu_optimized": true,
    "memory_optimized": true,
    "batch_size": 2,
    "sequence_length": 128,
    "epochs": 1,
    "adapter_output_path": "models/adapters/",
    "training_data_path": "data/synthetic/",
    "samples_per_domain": 5000,
    "current_training": "education (17% complete - 195/1125 steps)"
  },
  "configuration": {
    "version": "2.3-gguf-enhanced-space-tech",
    "default_fallback_model": "microsoft_DialoGPT-medium",
    "model_loading_timeout": 60,
    "enable_model_caching": true,
    "auto_fallback_on_error": true,
    "fallback_hierarchy": ["gguf_model", "primary_model", "fallback_model", "base_model"],
    "model_selection_strategy": "phase_aware_gguf_enhanced",
    "context_window_optimization": false,
    "memory_management": {
      "enable_model_offloading": false,
      "max_concurrent_models": 1,
      "memory_threshold_gb": 4,
      "cpu_optimized_loading": true,
      "gguf_optimization": true
    },
    "performance_monitoring": {
      "enable_metrics": true,
      "log_model_performance": true,
      "track_training_progress": true,
      "monitor_memory_usage": true,
      "phase_progression_tracking": true,
      "gguf_performance_tracking": true
    },
    "trinity_architecture": {
      "phase_1_arc_reactor": "5 domains - CPU optimized foundation",
      "phase_2_perplexity": "10 domains - Context-aware intelligence with GGUF enhancement",
      "phase_3_einstein": "8 domains - Fusion mathematics with advanced GGUF models",
      "phase_4_universal": "4 domains - Complete Trinity integration with flagship GGUF models"
    },
    "space_technology_focus": {
      "research_domains": ["research", "engineering", "data_science"],
      "innovation_domains": ["innovation", "social_impact", "global_awareness"],
      "primary_gguf_model": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
      "specializations": ["aerospace_engineering", "space_propulsion", "orbital_mechanics", "satellite_systems", "mission_planning", "space_colonization"]
    },
    "meetara_integration": {
      "tara_port": 5000,
      "meetara_ports": [2025, 8765, 8766],
      "integration_ready": false,
      "phase_1_foundation": true,
      "full_vision_domains": 24,
      "gguf_enhanced": true
    },
    "last_updated": "2025-01-23",
    "schema_version": "2.3-gguf-enhanced-space-tech"
  },
  "model_status_legend": {
    "ready": "Model is downloaded and ready for use",
    "available": "Model is available but not actively used",
    "training_active": "Model/adapter currently being trained",
    "training_pending": "Model/adapter queued for training in current phase",
    "phase_2_planned": "Model/adapter planned for Phase 2 with GGUF enhancement",
    "phase_3_planned": "Model/adapter planned for Phase 3 with advanced GGUF models",
    "phase_4_planned": "Model/adapter planned for Phase 4 with flagship GGUF models",
    "phase_2_target": "Target model for Phase 2 domains",
    "phase_2_3_target": "Target model for Phase 2-3 domains",
    "phase_3_4_target": "Target model for Phase 3-4 domains including space technology",
    "cpu_optimized": "Optimized for CPU-only training environments",
    "gguf_enhanced": "Enhanced with quantized GGUF models for better performance"
  },
  "generic_to_actual": {
    "Premium-8B-Instruct": "meta-llama/Llama-3.1-8B-Instruct",
    "Technical-3.8B-Instruct": "microsoft/Phi-3.5-mini-instruct",
    "Efficient-1B-Instruct": "meta-llama/Llama-3.2-1B-instruct",
    "DialoGPT-medium": "microsoft/DialoGPT-medium",
    "Qwen2.5-3B-Instruct": "Qwen/Qwen2.5-3B-Instruct"
  },
  "actual_to_generic": {
    "meta-llama/Llama-3.1-8B-Instruct": "Premium-8B-Instruct",
    "microsoft/Phi-3.5-mini-instruct": "Technical-3.8B-Instruct",
    "meta-llama/Llama-3.2-1B-instruct": "Efficient-1B-Instruct",
    "microsoft/DialoGPT-medium": "DialoGPT-medium",
    "Qwen/Qwen2.5-3B-Instruct": "Qwen2.5-3B-Instruct"
  },
  "parameters": {
    "Premium-8B-Instruct": 8000000000,
    "Technical-3.8B-Instruct": 3800000000,
    "Efficient-1B-Instruct": 1000000000,
    "DialoGPT-medium": 345000000,
    "Qwen2.5-3B-Instruct": 3000000000
  },
  "domains": {
    "healthcare": {
      "optimal_model": "DialoGPT-medium",
      "reason": "Therapeutic communication, empathy, conversational flow"
    },
    "business": {
      "optimal_model": "Premium-8B-Instruct",
      "reason": "Complex reasoning, strategic thinking, multi-step planning"
    },
    "education": {
      "optimal_model": "Technical-3.8B-Instruct",
      "reason": "Technical knowledge, instruction following, creative problem-solving"
    },
    "creative": {
      "optimal_model": "Technical-3.8B-Instruct",
      "reason": "Technical knowledge, instruction following, creative problem-solving"
    },
    "leadership": {
      "optimal_model": "Premium-8B-Instruct",
      "reason": "Complex reasoning, strategic thinking, multi-step planning"
    }
  }
} 